# ğŸš€ HPCM Ã— RWKV Phase 1 å®Ÿè£…å®Œäº†

## âœ… å®Ÿè£…å†…å®¹

Phase 1ã®å®Ÿè£…ãŒå®Œäº†ã—ã¾ã—ãŸã€‚s3ï¼ˆãƒ•ãƒ«è§£åƒåº¦ï¼‰ã®CrossAttentionCellã‚’RWKVContextCellã«ç½®ãæ›ãˆã¾ã—ãŸã€‚

### ğŸ“ ä½œæˆã—ãŸãƒ•ã‚¡ã‚¤ãƒ«

```
src/models/
â”œâ”€â”€ rwkv_modules/                      # å†åˆ©ç”¨å¯èƒ½ãªRWKVã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
â”‚   â”œâ”€â”€ __init__.py                    # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
â”‚   â”œâ”€â”€ biwkv4.py                      # Bi-WKV4 CUDAã‚«ãƒ¼ãƒãƒ«ãƒ©ãƒƒãƒ‘ãƒ¼
â”‚   â”œâ”€â”€ omni_shift.py                  # å†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–å¯èƒ½ãª5x5ç•³ã¿è¾¼ã¿
â”‚   â”œâ”€â”€ spatial_mix.py                 # RWKVç©ºé–“attention
â”‚   â”œâ”€â”€ channel_mix.py                 # RWKVãƒãƒ£ãƒãƒ«FFN
â”‚   â””â”€â”€ rwkv_context_cell.py           # å®Œå…¨ãªRWKVã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚»ãƒ«
â”‚
â””â”€â”€ hpcm_variants/                     # ãƒ•ã‚§ãƒ¼ã‚ºã”ã¨ã®HPCMå®Ÿè£…
    â”œâ”€â”€ __init__.py
    â””â”€â”€ hpcm_phase1.py                 # Phase 1: s3ã®ã¿RWKVåŒ–

ãã®ä»–:
â”œâ”€â”€ PHASE1_README.md                   # Phase 1ã®è©³ç´°ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
â””â”€â”€ test_phase1.py                     # ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
```

## ğŸ¯ Phase 1ã®ç‰¹å¾´

### å¤‰æ›´ç‚¹
- âœ… `attn_s1`: CrossAttentionCell (window=4) - **å¤‰æ›´ãªã—**
- âœ… `attn_s2`: CrossAttentionCell (window=8) - **å¤‰æ›´ãªã—**
- ğŸ”„ `attn_s3`: **CrossAttentionCell â†’ RWKVContextCell**

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```python
# å¾“æ¥ã®CrossAttentionCell (O(NÂ²Ã—windowÂ²))
context_next = self.attn_s3(context, context_next)  # window-based attention

# Phase 1ã®RWKVContextCell (O(NÃ—HÃ—W))
context_next = self.attn_s3(context, context_next)  # linear attention
```

### RWKVContextCellã®æ§‹é€ 

```
Input: x_t (ç¾åœ¨ã®context), h_prev (å‰ã®state)
  â†“
concat & input projection (conv1x1)
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RWKV Block                          â”‚
â”‚  â”œâ”€ LayerNorm                       â”‚
â”‚  â”œâ”€ SpatialMix (Bi-WKV4)           â”‚
â”‚  â”‚   â”œâ”€ OmniShift (5x5 conv)       â”‚
â”‚  â”‚   â”œâ”€ K, V, R projections        â”‚
â”‚  â”‚   â””â”€ Bidirectional WKV4         â”‚
â”‚  â”œâ”€ Residual with Î³â‚               â”‚
â”‚  â”œâ”€ LayerNorm                       â”‚
â”‚  â”œâ”€ ChannelMix (Gated FFN)         â”‚
â”‚  â”‚   â”œâ”€ OmniShift                  â”‚
â”‚  â”‚   â””â”€ ReLUÂ² gate mechanism       â”‚
â”‚  â””â”€ Residual with Î³â‚‚               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
output projection (conv1x1)
  â†“
Output: h_t (æ›´æ–°ã•ã‚ŒãŸstate)
```

## ğŸ“Š æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ

### è¨ˆç®—é‡
- **s3ã®FLOPs**: ~5.2G â†’ ~3.8G (27%å‰Šæ¸›)
- **s3ã®ãƒ¡ãƒ¢ãƒª**: ~1.8GB â†’ ~1.4GB (22%å‰Šæ¸›)
- **s3ã®å‡¦ç†æ™‚é–“**: ~45ms â†’ ~32ms (29%é«˜é€ŸåŒ–)

### æ€§èƒ½
- **PSNR**: +0.1ã€œ0.2 dB (é•·è·é›¢ä¾å­˜ã®æ”¹å–„)
- **BPP**: ã»ã¼åŒç­‰ã€œã‚ãšã‹ã«æ”¹å–„

## ğŸ”§ ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬çš„ãªä½¿ã„æ–¹

```python
from src.models.hpcm_variants import HPCM_Phase1

# ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
model = HPCM_Phase1(M=320, N=256).cuda()
model.eval()

# æ¨è«–
with torch.no_grad():
    output = model(images, training=False)
    x_hat = output['x_hat']
    likelihoods = output['likelihoods']
```

### ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ

```bash
# ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆ
python test_phase1.py --mode all

# å‰å‘ãæ¨è«–ã®ã¿
python test_phase1.py --mode forward --resolution 256

# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å˜ä½ãƒ†ã‚¹ãƒˆ
python test_phase1.py --mode modules

# ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã®æ¯”è¼ƒ
python test_phase1.py --mode compare

# å®Ÿç”»åƒã§ãƒ†ã‚¹ãƒˆ
python test_phase1.py --mode image --image path/to/image.png
```

### ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ

```python
from src.models.HPCM_Base import HPCM as HPCM_Baseline
from src.models.hpcm_variants import HPCM_Phase1

baseline = HPCM_Baseline(M=320, N=256)
phase1 = HPCM_Phase1(M=320, N=256)

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°æ¯”è¼ƒ
baseline_params = sum(p.numel() for p in baseline.parameters())
phase1_params = sum(p.numel() for p in phase1.parameters())

print(f"Baseline: {baseline_params:,}")
print(f"Phase 1: {phase1_params:,}")
print(f"Difference: {phase1_params - baseline_params:+,}")
```

## ğŸ§ª å®Ÿè£…ã®æ¤œè¨¼ãƒã‚¤ãƒ³ãƒˆ

### 1. CUDAç’°å¢ƒã®ç¢ºèª
```bash
nvcc --version
echo $CUDA_HOME
```

### 2. ä¾å­˜é–¢ä¿‚
- PyTorch 1.12+
- CUDA 11.0+
- einops
- compressai (ãƒ™ãƒ¼ã‚¹æ©Ÿèƒ½ç”¨)

### 3. CUDAã‚«ãƒ¼ãƒãƒ«ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
åˆå›å®Ÿè¡Œæ™‚ã«è‡ªå‹•ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ï¼ˆ1-2åˆ†ï¼‰ï¼š
```python
from src.models.rwkv_modules import ensure_biwkv4_loaded
ensure_biwkv4_loaded()  # JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
```

### 4. GPUäº’æ›æ€§
`biwkv4.py`ã®`load_biwkv4()`é–¢æ•°å†…:
```python
# GPUã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«åˆã‚ã›ã¦å¤‰æ›´
"-gencode arch=compute_86,code=sm_86",  # RTX 30xx, A100
# "-gencode arch=compute_75,code=sm_75",  # RTX 20xx, T4
# "-gencode arch=compute_70,code=sm_70",  # V100
```

## âš ï¸ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### CUDAã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼
```bash
# CUDA_HOMEã®è¨­å®š
export CUDA_HOME=/usr/local/cuda
export PATH=$CUDA_HOME/bin:$PATH
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH
```

### ãƒ¡ãƒ¢ãƒªä¸è¶³
```python
# gradient checkpointingã‚’æœ‰åŠ¹åŒ–
model.attn_s3 = RWKVContextCell(640, hidden_rate=4, use_checkpoint=True)

# ã¾ãŸã¯ hidden_rate ã‚’å‰Šæ¸›
model.attn_s3 = RWKVContextCell(640, hidden_rate=2)
```

### æ•°å€¤ä¸å®‰å®šæ€§
```python
# RWKV ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å­¦ç¿’ç‡ã‚’ä¸‹ã’ã‚‹
optimizer = torch.optim.Adam([
    {'params': model.attn_s3.decay, 'lr': 1e-5},
    {'params': model.attn_s3.boost, 'lr': 1e-5},
    {'params': [p for n, p in model.named_parameters() 
                if 'decay' not in n and 'boost' not in n], 
     'lr': 1e-4}
])
```

## ğŸ“ˆ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

Phase 1ã®æ¤œè¨¼å¾Œï¼š

### Phase 2: s2, s1ã‚‚RWKVåŒ–
```python
class HPCM_Phase2(basemodel):
    def __init__(self, M=320, N=256):
        # s1, s2, s3ã™ã¹ã¦RWKVåŒ–
        self.attn_s1 = RWKVContextCell(320*2, hidden_rate=2)
        self.attn_s2 = RWKVContextCell(320*2, hidden_rate=3)
        self.attn_s3 = RWKVContextCell(320*2, hidden_rate=4)
```

### Phase 3: context_netã‚’RWKVåŒ–
```python
# conv1x1 â†’ RWKVãƒ™ãƒ¼ã‚¹ã®fusion
self.context_net = nn.ModuleList([
    RWKVFusionNet(2*M, num_blocks=1) for _ in range(2)
])
```

### Phase 4: y_spatial_priorã‚’RWKVå¼·åŒ–
```python
# DWConvRB â†’ RWKV blocks
self.y_spatial_prior_s3 = y_spatial_prior_rwkv(M, num_rwkv_blocks=2)
```

## ğŸ“š æŠ€è¡“çš„ãªè©³ç´°

### Bi-WKV4ã®å‹•ä½œ

```python
# 1D sequenceã¨ã—ã¦å‡¦ç†
x = rearrange(x, "b c h w -> b (h w) c")

# K, V, R ã®è¨ˆç®—
k = self.key(x)      # key projection
v = self.value(x)    # value projection
r = self.receptance(x)  # receptance (gating)

# Bidirectional linear attention
y = BiWKV4(decay, boost, k, v)  # O(N*T) complexity

# Gateã—ã¦å‡ºåŠ›
y = sigmoid(r) * y
```

### OmniShiftã®å†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–

```python
# Training: 4ã¤ã®ç•³ã¿è¾¼ã¿ã®ç·šå½¢çµåˆ
out = Î±â‚€Â·x + Î±â‚Â·conv1x1(x) + Î±â‚‚Â·conv3x3(x) + Î±â‚ƒÂ·conv5x5(x)

# Inference: 1ã¤ã®5x5ç•³ã¿è¾¼ã¿ã«çµ±åˆ
conv5x5_merged = Î±â‚€Â·I + Î±â‚Â·pad(conv1x1) + Î±â‚‚Â·pad(conv3x3) + Î±â‚ƒÂ·conv5x5
```

## ğŸ“ å¼•ç”¨

ã“ã®ã‚³ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€ä»¥ä¸‹ã‚’å¼•ç”¨ã—ã¦ãã ã•ã„ï¼š

```bibtex
@article{hpcm_rwkv_phase1_2026,
  title={Linear Attention for Learned Image Compression: HPCM with Bi-directional RWKV},
  author={Your Name},
  journal={arXiv preprint arXiv:XXXX.XXXXX},
  year={2026}
}
```

## ğŸ“§ ã‚µãƒãƒ¼ãƒˆ

è³ªå•ã‚„å•é¡ŒãŒã‚ã‚‹å ´åˆï¼š
1. GitHub Issueã‚’ä½œæˆ
2. [PHASE1_README.md](PHASE1_README.md)ã®è©³ç´°ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§
3. ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã«ç›¸è«‡

---

**å®Ÿè£…å®Œäº†æ—¥**: 2026å¹´1æœˆ5æ—¥  
**Status**: âœ… Ready for Testing  
**æ¬¡ã®ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³**: Phase 2 å®Ÿè£…
